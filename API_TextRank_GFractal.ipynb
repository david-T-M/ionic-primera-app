{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "API_TextRank_GFractal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOZ3kohPg4bFdCAwTFefoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-T-M/ionic-primera-app/blob/main/API_TextRank_GFractal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQif70fBa5lC"
      },
      "source": [
        "#\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KegPrwMkeY_B",
        "outputId": "9b3e0343-bc60-47f8-df2c-1d944aecba8d"
      },
      "source": [
        "!pip install deplacy\r\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deplacy\n",
            "  Downloading https://files.pythonhosted.org/packages/42/92/f746b19cc9ca622f0acdf484c78c9ffb5723155afe74d41a6f89984142f0/deplacy-1.9.2-py3-none-any.whl\n",
            "Installing collected packages: deplacy\n",
            "Successfully installed deplacy-1.9.2\n",
            "Collecting es_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp37-none-any.whl size=16172936 sha256=30219061cb325f0b2d623f41ad166b051f7a9079a361cb953e62c86f3407320f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-goh72aga/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrTBJWzxepw-",
        "outputId": "08bfa912-adf7-4a47-88c2-011551d3c3d5"
      },
      "source": [
        "import pkg_resources,imp\r\n",
        "imp.reload(pkg_resources)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'pkg_resources' from '/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOwjehIscqgG"
      },
      "source": [
        "Librerías necesarias para los algoritmos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVh4oQV0brbL"
      },
      "source": [
        "from math import *\r\n",
        "from math import sqrt\r\n",
        "import string\r\n",
        "import operator\r\n",
        "import random\r\n",
        "import pandas as pd\r\n",
        "#librerias necesarias para text rank\r\n",
        "from collections import OrderedDict\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "\r\n",
        "#Listado de STOPWORDS dependiendo del lenguaje\r\n",
        "#from spacy.lang.en.stop_words import STOP_WORDS\r\n",
        "from spacy.lang.es.stop_words import STOP_WORDS\r\n",
        "\r\n",
        "#nlp = spacy.load('en_core_web_sm')\r\n",
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vW8QRTHbrzW"
      },
      "source": [
        "TextRank\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNqo78cRbuEO"
      },
      "source": [
        "class TextRank4Keyword():\r\n",
        "    \"\"\"Extract keywords from text\"\"\"\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.d = 0.85 # damping coefficient, usually is .85\r\n",
        "        self.min_diff = 1e-5 # convergence threshold\r\n",
        "        self.steps = 100 # iteration steps\r\n",
        "        self.node_weight = None # save keywords and its weight\r\n",
        "\r\n",
        "    \r\n",
        "    def set_stopwords(self, stopwords):  \r\n",
        "        \"\"\"Set stop words\"\"\"\r\n",
        "        for word in STOP_WORDS.union(set(stopwords)):\r\n",
        "            lexeme = nlp.vocab[word]\r\n",
        "            lexeme.is_stop = True\r\n",
        "    \r\n",
        "    def sentence_segment(self, doc, candidate_pos, lower):\r\n",
        "        \"\"\"Store those words only in cadidate_pos\"\"\"\r\n",
        "        sentences = []\r\n",
        "        for sent in doc.sents:\r\n",
        "            selected_words = []\r\n",
        "            for token in sent:\r\n",
        "                # Store words only with cadidate POS tag\r\n",
        "                if token.pos_ in candidate_pos and token.is_stop is False:\r\n",
        "                    if lower is True:\r\n",
        "                        selected_words.append(token.text.lower())\r\n",
        "                    else:\r\n",
        "                        selected_words.append(token.text)\r\n",
        "            sentences.append(selected_words)\r\n",
        "        return sentences\r\n",
        "        \r\n",
        "    def get_vocab(self, sentences):\r\n",
        "        \"\"\"Get all tokens\"\"\"\r\n",
        "        vocab = OrderedDict()\r\n",
        "        i = 0\r\n",
        "        for sentence in sentences:\r\n",
        "            for word in sentence:\r\n",
        "                if word not in vocab:\r\n",
        "                    vocab[word] = i\r\n",
        "                    i += 1\r\n",
        "        return vocab\r\n",
        "    \r\n",
        "    def get_token_pairs(self, window_size, sentences):\r\n",
        "        \"\"\"Build token_pairs from windows in sentences\"\"\"\r\n",
        "        token_pairs = list()\r\n",
        "        for sentence in sentences:\r\n",
        "            for i, word in enumerate(sentence):\r\n",
        "                for j in range(i+1, i+window_size):\r\n",
        "                    if j >= len(sentence):\r\n",
        "                        break\r\n",
        "                    pair = (word, sentence[j])\r\n",
        "                    if pair not in token_pairs:\r\n",
        "                        token_pairs.append(pair)\r\n",
        "        return token_pairs\r\n",
        "        \r\n",
        "    def symmetrize(self, a):\r\n",
        "        return a + a.T - np.diag(a.diagonal())\r\n",
        "    \r\n",
        "    def get_matrix(self, vocab, token_pairs):\r\n",
        "        \"\"\"Get normalized matrix\"\"\"\r\n",
        "        # Build matrix\r\n",
        "        vocab_size = len(vocab)\r\n",
        "        g = np.zeros((vocab_size, vocab_size), dtype='float')\r\n",
        "        for word1, word2 in token_pairs:\r\n",
        "            i, j = vocab[word1], vocab[word2]\r\n",
        "            g[i][j] = 1\r\n",
        "            \r\n",
        "        # Get Symmeric matrix\r\n",
        "        g = self.symmetrize(g)\r\n",
        "        \r\n",
        "        # Normalize matrix by column\r\n",
        "        norm = np.sum(g, axis=0)\r\n",
        "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\r\n",
        "        \r\n",
        "        return g_norm\r\n",
        "\r\n",
        "    \r\n",
        "    def get_keywords(self, number=10):\r\n",
        "        \"\"\"Print top number keywords\"\"\"\r\n",
        "        keysw={}\r\n",
        "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\r\n",
        "        for i, (key, value) in enumerate(node_weight.items()):\r\n",
        "            keysw[key] =value\r\n",
        "            if i > number:\r\n",
        "                break\r\n",
        "        return keysw\r\n",
        "        \r\n",
        "        \r\n",
        "    def analyze(self, text, \r\n",
        "                candidate_pos=['NOUN', 'PROPN'], \r\n",
        "                window_size=4, lower=False, stopwords=list()):\r\n",
        "        \"\"\"Main function to analyze text\"\"\"\r\n",
        "        \r\n",
        "        # Set stop words\r\n",
        "        self.set_stopwords(stopwords)\r\n",
        "        \r\n",
        "        # Pare text by spaCy\r\n",
        "        doc = nlp(text)\r\n",
        "        \r\n",
        "        # Filter sentences\r\n",
        "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\r\n",
        "        \r\n",
        "        # Build vocabulary\r\n",
        "        vocab = self.get_vocab(sentences)\r\n",
        "        \r\n",
        "        # Get token_pairs from windows\r\n",
        "        token_pairs = self.get_token_pairs(window_size, sentences)\r\n",
        "        \r\n",
        "        # Get normalized matrix\r\n",
        "        g = self.get_matrix(vocab, token_pairs)\r\n",
        "        \r\n",
        "        # Initionlization for weight(pagerank value)\r\n",
        "        pr = np.array([1] * len(vocab))\r\n",
        "        \r\n",
        "        # Iteration\r\n",
        "        previous_pr = 0\r\n",
        "        for epoch in range(self.steps):\r\n",
        "            pr = (1-self.d) + self.d * np.dot(g, pr)\r\n",
        "            if abs(previous_pr - sum(pr))  < self.min_diff:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                previous_pr = sum(pr)\r\n",
        "\r\n",
        "        # Get weight for each node\r\n",
        "        node_weight = dict()\r\n",
        "        for word, index in vocab.items():\r\n",
        "            node_weight[word] = pr[index]\r\n",
        "        \r\n",
        "        self.node_weight = node_weight\r\n",
        "        "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqG-EIfg_2P"
      },
      "source": [
        "Grado de fractalidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKvAbsbrb68T"
      },
      "source": [
        "def fractalidad(palabras,vocabulario,frec,dist):\r\n",
        "    N=len(palabras)                                     #El número de tokens de todo el texto\r\n",
        "    gf={}\r\n",
        "    cajas_index=set()\r\n",
        "    voc=[]                                             #la variable voc contendra cada sintagma con frecuencia mayor que 1, por que las otras palabras tendrán 0 de grado de fractaldiad\r\n",
        "    for p in vocabulario:                              #Esto se puede hacer fuera del algoritmo, pero se incluye para evitar ese calculo innecesario \r\n",
        "        if(p not in voc):\r\n",
        "            #if(frec[p]>1):\r\n",
        "              # if(p not in STOP_WORDS):\r\n",
        "              #      if(len(p)>1):\r\n",
        "                        voc.append(p)\r\n",
        "    print(\"Text size: \",N)\r\n",
        "    print(\"Vocabulary: \",len(voc))\r\n",
        "    for p in voc:                                  \r\n",
        "        rcajas=dist[p]\r\n",
        "        M=frec[p]                                  \r\n",
        "        dfw=0.0\r\n",
        "        nsh=0.0\r\n",
        "        for s in range(1,N+1):  \r\n",
        "            noc=0                                       \r\n",
        "            for e in rcajas:                       \r\n",
        "                cajas_index.add(ceil(int(e)/s))    \r\n",
        "            noc=len(cajas_index)                    \r\n",
        "            cajas_index.clear()    \r\n",
        "            ns=N/s\r\n",
        "            if(M<=ns):\r\n",
        "                nsh=M\r\n",
        "            else:\r\n",
        "                nsh=M/(1+(M-1)/(N-1)*(s-1)) \r\n",
        "            dfw=dfw+fabs(log(nsh/noc))\r\n",
        "        gf[p]=dfw\r\n",
        "    return gf    #regresamos un diccionario"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYUYiEhbu8Iz"
      },
      "source": [
        "def distribucion(palabras,vocabulario):\r\n",
        "    N=len(palabras)\r\n",
        "    ncajas=[]\r\n",
        "    cajas={}\r\n",
        "    frecuencias={}\r\n",
        "    for p in vocabulario:\r\n",
        "        ncajas.clear()\r\n",
        "        i=0\r\n",
        "        M=palabras.count(p)\r\n",
        "        while(i<N):\r\n",
        "            if(p == palabras[i]):\r\n",
        "                ncajas.append(i+1)\r\n",
        "            i=i+1\r\n",
        "        frecuencias[p]=M\r\n",
        "        cajas[p]=ncajas[:]\r\n",
        "    return frecuencias,cajas,N"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YBNXt0zwy3R"
      },
      "source": [
        "def salida(sorted_x,frec,archivo,elapsed_time,N,L):\r\n",
        "  dfx=pd.DataFrame([[str(t[0]),str(frec[t[0]]), str(t[1]), str((t[1])*log10(frec[t[0]]))] for t in sorted_x] , columns=['word','frecuency','Degree_of_fractality','Combined_measured'])\r\n",
        "  return dfx"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Eekk017xeq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANaeOolE7xw4"
      },
      "source": [
        "DEFINICIÓN DEL NOMBRE DEL ARCHIVO A PROCESAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkCfLCCDvFM_"
      },
      "source": [
        "filename='data.txt'\r\n",
        "texto=cargar_datos(filename)\r\n",
        "#obtenemos el vocabulario\r\n",
        "tokens=texto.split()\r\n",
        "vocabulario=[]\r\n",
        "for t in tokens:\r\n",
        "    if(t not in vocabulario):\r\n",
        "        vocabulario.append(t)\r\n",
        "#variables de procesamiento\r\n",
        "dist={}\r\n",
        "frec={}\r\n",
        "frec,dist,tamanio_texto=distribucion(tokens,vocabulario)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JlyILQOcvVz"
      },
      "source": [
        "Lectura de documentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eys2G4z8b-zK"
      },
      "source": [
        "#Lectura de archivo para generación de vocabulario\r\n",
        "def cargar_datos(filename):\r\n",
        "    f=open('/content/InputData/'+filename, \"r\") #tenemos que crear un directorio llamado InputData\r\n",
        "    texto=f.read()\r\n",
        "    #Pasar a minusculas\r\n",
        "    texto=texto.lower()\r\n",
        "    #Eliminar puntuación\r\n",
        "    texto=texto.translate(str.maketrans('', '', string.punctuation))\r\n",
        "    texto=texto.translate(str.maketrans('', '', '¿¡—“”0123456789’'))\r\n",
        "    palabras=texto.split()\r\n",
        "    textop=\"\"\r\n",
        "    #rearmamos el texto debido a ue existen carácteres especiales\r\n",
        "    for w in palabras:\r\n",
        "        textop=textop+w+' '\r\n",
        "    return textop"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht4genjTyIar"
      },
      "source": [
        "Ejecución de algoritmos\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI8zOnFZcYWe",
        "outputId": "919be76a-6ac8-4702-e556-4161c4f75b50"
      },
      "source": [
        "#ejecución de algoritmo Grado de Fractalidad\r\n",
        "from time import time\r\n",
        "start_time = time()\r\n",
        "frac_x=fractalidad(tokens,vocabulario,frec,dist) #solamente se calcular el grado de fractalidad de las palabras que tengan mas de uno de frecuencia\r\n",
        "elapsed_time = time() - start_time \r\n",
        "sorted_x = sorted(frac_x.items(), key=operator.itemgetter(1), reverse=True)\r\n",
        "print('Time GF: '+str(elapsed_time))\r\n",
        "\r\n",
        "#Imprimir y guardar resultados de GF\r\n",
        "df=salida(sorted_x,frec,filename,elapsed_time,tamanio_texto, len(vocabulario))\r\n",
        "by_MC = df.sort_values('Combined_measured',ascending=False)\r\n",
        "by_MC.to_csv('/content/Results/GF.csv')\r\n",
        "\r\n",
        "\r\n",
        "#ejecución de algoritmo de TextRank\r\n",
        "start_time = time()\r\n",
        "tr4w = TextRank4Keyword()\r\n",
        "tr4w.analyze(texto, candidate_pos = ['NOUN','PROPN'], window_size=4, lower=False)\r\n",
        "kwTR=tr4w.get_keywords(100)\r\n",
        "\r\n",
        "#Guardar resultados de TextRank\r\n",
        "dftr=pd.DataFrame([[key, kwTR[key]] for key in kwTR.keys()], columns=['word', 'Index'])\r\n",
        "elapsed_time = time() - start_time\r\n",
        "print('Time TextRank: '+str(elapsed_time))\r\n",
        "dftr.to_csv('/content/Results/TextRank.csv')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text size:  98\n",
            "Vocabulary:  77\n",
            "Time GF: 0.00758814811706543\n",
            "Time TextRank: 0.03065967559814453\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}